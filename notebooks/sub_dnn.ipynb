{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "eqAdMszb3MaD"
   },
   "source": [
    "import scipy.io\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_regression as MIR\n",
    "\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import datetime\n",
    "import pickle\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2kgD3iS83MaE"
   },
   "source": [
    "def calc_data_sent(t_0, t_i, gh, gt, D, W, eff, P_AP, N0, p_max):\n",
    "    energy_harvested = t_0 * eff * P_AP * gh\n",
    "    power_to_comsume_energy = energy_harvested / t_i\n",
    "    power_to_comsume_energy[power_to_comsume_energy > p_max] = p_max\n",
    "    data_sent = t_i * W * np.log2(1 + (power_to_comsume_energy * gt) / (N0 * W))\n",
    "    return data_sent"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9nTrnu5o3MaE"
   },
   "source": [
    "def outage_calc(t_0_dim, t_i, gh, gt, D, W, eff, P_AP, N0, p_max):\n",
    "    t_0 = t_0_dim.reshape(len(t_0_dim), 1)\n",
    "    energy_harvested = t_0 * eff * P_AP * gh\n",
    "    power_to_comsume_energy = energy_harvested / t_i\n",
    "    power_to_comsume_energy[power_to_comsume_energy > p_max] = p_max\n",
    "    data_sent = t_i * W * torch.log2(1 + (power_to_comsume_energy * gt) / (N0 * W))\n",
    "    outage = D - data_sent\n",
    "    # outage[outage<0]=0\n",
    "    outage = torch.abs(outage)\n",
    "    return torch.sum(outage)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xNjKWnlr3MaE"
   },
   "source": [
    "def loss_fn(t_0, output_time, tgt_time, gh, gt):\n",
    "    mse_loss = nn.MSELoss()\n",
    "    outage = outage_calc(\n",
    "        t_0, output_time / scale_factor, gh, gt, D, W, eff, P_AP, N0, p_max\n",
    "    )\n",
    "    mse = mse_loss(output_time, tgt_time)\n",
    "    return 10 * outage, outage"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S5yqHzxa3MaE"
   },
   "source": [
    "input_data_folder = '../input_data'\n",
    "output_data_folder = '../output/sub_dnn_models/'\n",
    "tensor_board_folder = '../runs/sub_dnn'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OMxIwGZV3MaE"
   },
   "source": [
    "## Fixed params\n",
    "eff = 0.5\n",
    "nu = 1\n",
    "PL_d0_dB = 31.67\n",
    "alpha = 2\n",
    "Z_stdDev = 2\n",
    "W = 1e6\n",
    "N0 = 1e-14\n",
    "D = 50\n",
    "\n",
    "scale_factor = 1e7\n",
    "numChReal = 1000000\n",
    "## params changing based on input file\n",
    "P_AP = 2\n",
    "p_max = 0.01\n",
    "num_of_user = 4\n",
    "train_data_size=100"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hCV4Q8G43MaF"
   },
   "source": [
    "##  LOAD DATA\n",
    "match = scipy.io.loadmat(input_data_folder + \"/channel_gains.mat\")\n",
    "test_ind=np.load(input_data_folder+'/test_ind.npy')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "71uZBxMj3MaG",
    "outputId": "27b1a6f9-b371-423b-9f69-b51808337583"
   },
   "source": [
    "feat_dict={}\n",
    "for num_of_user in range(2,5,2):\n",
    "    val_loss_list = []\n",
    "    ## DNN Arch  ##\n",
    "    class SubNet(nn.Module):\n",
    "\n",
    "        def __init__(self):\n",
    "            super(SubNet, self).__init__()\n",
    "            self.layer1 = nn.Linear(3 * num_of_user + 1, 8 * num_of_user)\n",
    "            self.layer2 = nn.Linear(8 * num_of_user, 8 * num_of_user)\n",
    "            self.layer3 = nn.Linear(8 * num_of_user, 8 * num_of_user)\n",
    "            self.layer4 = nn.Linear(8 * num_of_user, 4 * num_of_user)\n",
    "            self.layer5 = nn.Linear(4 * num_of_user, 4 * num_of_user)\n",
    "            self.output = nn.Linear(4 * num_of_user, num_of_user)\n",
    "\n",
    "        def forward(self, input_x):\n",
    "            x = F.relu(self.layer1(input_x))\n",
    "            x = F.relu(self.layer2(x))\n",
    "            x = F.relu(self.layer3(x))\n",
    "            x = F.relu(self.layer4(x))\n",
    "            x = F.relu(self.layer5(x))\n",
    "\n",
    "            x = torch.abs(self.output(x))\n",
    "            return x\n",
    "\n",
    "    ## Data Load\n",
    "    mat = scipy.io.loadmat(\n",
    "        input_data_folder\n",
    "        + \"/v2_subdnn_label_N014\"\n",
    "        + \"_PAP\"\n",
    "        + str(P_AP)\n",
    "        + \"_M\"\n",
    "        + str(num_of_user)\n",
    "        + \"_pmax\"\n",
    "        + str(p_max)\n",
    "        + \".mat\"\n",
    "    )\n",
    "\n",
    "    gh = match[\"gh_arr\"][0:num_of_user, 0:numChReal]\n",
    "    gt = match[\"gt_arr\"][0:num_of_user, 0:numChReal]\n",
    "\n",
    "    gamma = gh * gt * eff * P_AP / (W * N0)\n",
    "    alpha = np.abs(scipy.special.lambertw(np.exp(-1) * (gamma - 1), k=0) + 1)\n",
    "\n",
    "    df = pd.DataFrame(mat[\"label_time_for_sub_dnn\"])\n",
    "    df[\"ind\"] = mat[\"ind_arr\"][:, 0]\n",
    "    # df2=pd.DataFrame(np.concatenate((gh,gt,alpha,gamma,beta,theta)).T)\n",
    "    df2 = pd.DataFrame(np.concatenate((gh, gt, alpha, gamma)).T)\n",
    "    df2.set_index(df2.index.values + 1)\n",
    "    df = df.merge(df2, right_index=True, left_on=\"ind\")\n",
    "    df_test = df.loc[df[\"ind\"].isin(test_ind)]\n",
    "    df_train = df.loc[~df[\"ind\"].isin(test_ind)]\n",
    "\n",
    "    input_columns = [0] + list(np.arange(num_of_user + 2, df.shape[1]))\n",
    "    output_columns = np.arange(1, num_of_user + 1)\n",
    "\n",
    "    X_test = df_test.iloc[:, input_columns].values\n",
    "    y_test = df_test.iloc[:, output_columns].values\n",
    "    y_test = y_test * scale_factor\n",
    "    X_train = df_train.iloc[:, input_columns].values\n",
    "    y_train = df_train.iloc[:, output_columns].values\n",
    "    y_train = y_train * scale_factor\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.01, random_state=42\n",
    "    )\n",
    "\n",
    "    # Scale\n",
    "    ss = StandardScaler()\n",
    "    X_train = ss.fit_transform(X_train)\n",
    "    X_val = ss.transform(X_val)\n",
    "    X_test = ss.transform(X_test)\n",
    "    pickle.dump(ss, open(output_data_folder+'scaler_N'+str(num_of_user)+'.pkl','wb'))\n",
    "\n",
    "    # Feature Selection\n",
    "    mi = MIR(X_train[0:10000, 1:], y_train[0:10000].sum(axis=1))\n",
    "    feature_index = np.argsort(mi)\n",
    "    feat_size = 3 * num_of_user\n",
    "\n",
    "    feature_index = np.argsort(mi)\n",
    "    feature_index = [0] + list(feature_index[-1 * feat_size :] + 1)\n",
    "    feat_dict[num_of_user]=feature_index\n",
    "    \n",
    "\n",
    "    # Dataset & DataLoder\n",
    "    torch_dataset = TensorDataset(\n",
    "        torch.tensor(X_train[0:train_data_size, :].astype(np.float32)),\n",
    "        torch.tensor(y_train[0:train_data_size, :].astype(np.float32)),\n",
    "    )\n",
    "    val_torch_dataset = TensorDataset(\n",
    "        torch.tensor(X_val.astype(np.float32)), torch.tensor(y_val.astype(np.float32))\n",
    "    )\n",
    "\n",
    "    train_data_loader = DataLoader(torch_dataset, batch_size=32)\n",
    "    val_data_loader = DataLoader(val_torch_dataset, batch_size=1)\n",
    "\n",
    "    ## TRAINING\n",
    "    # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "    writer = SummaryWriter(\n",
    "        tensor_board_folder\n",
    "        + \"/trainsize\"\n",
    "        + str(train_data_size)\n",
    "        + \"_M\"\n",
    "        + str(num_of_user)\n",
    "        + \"_PAP\"\n",
    "        + str(P_AP)\n",
    "        + \"_pmax\"\n",
    "        + str(p_max)\n",
    "    )\n",
    "    NO_EPOCHS = 20\n",
    "\n",
    "    val_loss_best = 10000000\n",
    "\n",
    "    model = SubNet()\n",
    "    optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, \"min\")\n",
    "    loss_fn2 = nn.MSELoss()\n",
    "    for epoch_idx in range(NO_EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        len_batches = 0\n",
    "        for ii, sample in enumerate(tqdm(train_data_loader)):\n",
    "            local_inp, local_tgt = sample\n",
    "            local_inp_inv = torch.Tensor(ss.inverse_transform(local_inp))\n",
    "            output = model(local_inp[:, feature_index])\n",
    "            # local_inp_inv = torch.Tensor(ss.inverse_transform(local_inp))\n",
    "            # output = model(local_inp[:,8:],local_inp_inv)\n",
    "            # loss = loss_fn(output,local_tgt)\n",
    "            [loss, outage] = loss_fn(\n",
    "                local_inp_inv[:, 0],\n",
    "                output,\n",
    "                local_tgt,\n",
    "                local_inp_inv[:, 1 : num_of_user + 1],\n",
    "                local_inp_inv[:, num_of_user + 1 : 2 * num_of_user + 1],\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().numpy()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_outage = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for jj, val_sample in enumerate(val_data_loader):\n",
    "                local_inp, local_tgt = val_sample\n",
    "                local_inp_inv = torch.Tensor(ss.inverse_transform(local_inp))\n",
    "                output = model(local_inp[:, feature_index])\n",
    "                # output = model(local_inp[:,8:],local_inp_inv)\n",
    "                val_loss += loss_fn2(output, local_tgt)\n",
    "                [temp_loss, val_outage_temp] = loss_fn(\n",
    "                    local_inp_inv[:, 0],\n",
    "                    output,\n",
    "                    local_tgt,\n",
    "                    local_inp_inv[:, 1 : num_of_user + 1],\n",
    "                    local_inp_inv[:, num_of_user + 1 : 2 * num_of_user + 1],\n",
    "                )\n",
    "                # val_loss += temp_loss\n",
    "                val_outage += val_outage_temp\n",
    "\n",
    "            val_loss /= jj + 1\n",
    "            val_outage /= jj + 1\n",
    "\n",
    "        if val_loss < val_loss_best:\n",
    "            val_loss_best = val_loss\n",
    "            torch.save(model, output_data_folder+'/model_N'+str(num_of_user)+'.pt')\n",
    "            # X_test_inverse = torch.Tensor(ss.inverse_transform(X_test))\n",
    "            # test_output = model((torch.tensor(X_test.astype(np.float32))),X_test_inverse)\n",
    "\n",
    "        # writer.add_scalar('val_loss', val_loss, epoch_idx)\n",
    "\n",
    "        # writer.add_scalar('training loss', epoch_loss / (ii+1),epoch_idx)\n",
    "        # writer.add_scalar('val_outage',val_outage,epoch_idx)\n",
    "        # writer.add_scalar('train_outage',outage / (ii+1),epoch_idx)\n",
    "    val_loss_list.append(val_loss)\n",
    "np.save(\n",
    "    output_data_folder+\"/sub_dnn\", val_loss_list\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UsWVymMD3MaH"
   },
   "source": [
    "def fixed_point_iteration(M, D, W, gamma, t0, err_tol):\n",
    "    tau = np.zeros(M)\n",
    "    for i in range(0, M):\n",
    "        ti_prev = 0\n",
    "        ti = 10\n",
    "        while np.abs(ti_prev - ti) > err_tol * 1e-3:\n",
    "            ti_prev = ti\n",
    "            ti = D[i] / (W * np.log2(1 + gamma[i] * (t0 / ti_prev)))\n",
    "        tau[i] = ti\n",
    "    return tau\n",
    "\n",
    "\n",
    "def bisectionSearch_dnn(\n",
    "    N,\n",
    "    betaThr,\n",
    "    C_constants,\n",
    "    lb,\n",
    "    ub,\n",
    "    errTol,\n",
    "    D,\n",
    "    gamma,\n",
    "    W,\n",
    "    dnn_ins,\n",
    "    std_scaler,\n",
    "    outpt_scale_factor,\n",
    "    dnn_model,\n",
    "):\n",
    "\n",
    "    insTau_0 = ub\n",
    "    tau = C_constants\n",
    "\n",
    "    while ub - lb > 2 * errTol:\n",
    "        insTau_0 = (ub + lb) / 2\n",
    "        ins = np.append(insTau_0, dnn_ins)\n",
    "        ins = std_scaler.transform(ins.reshape(1, len(ins)))\n",
    "        ins = ins[0, feat_dict[num_of_user]].reshape(1, 3 * num_of_user + 1)\n",
    "        dnn_out = model((torch.tensor(ins.astype(np.float32))))\n",
    "        dnn_out = dnn_out / scale_factor\n",
    "        tau = dnn_out.detach().numpy()[0]\n",
    "\n",
    "        # tau= fixed_point_iteration(N,D,W,gamma,insTau_0,errTol);\n",
    "        index = insTau_0 >= betaThr\n",
    "        tau[index] = C_constants[index]\n",
    "\n",
    "        # print('******')\n",
    "        # print(tau)\n",
    "        # print(tau2)\n",
    "\n",
    "        turev = 1\n",
    "        for i in range(0, N):\n",
    "            if insTau_0 < betaThr[i]:\n",
    "                turev = turev + gamma[i] * tau[i] * W / (\n",
    "                    2 ** (D[i] / (W * tau[i])) * W * tau[i]\n",
    "                    - W * tau[i]\n",
    "                    - 2 ** (D[i] / (W * tau[i])) * D[i] * np.log(2)\n",
    "                )\n",
    "        if turev >= 0:\n",
    "            ub = insTau_0\n",
    "            # print('ub updated')\n",
    "        if (turev <= 0) or (np.isnan(turev)):\n",
    "            lb = insTau_0\n",
    "\n",
    "    optimalEHPoint = insTau_0\n",
    "\n",
    "    return optimalEHPoint, tau\n",
    "\n",
    "\n",
    "def MRTTMA_dnn(\n",
    "    K,\n",
    "    N,\n",
    "    D,\n",
    "    p_max,\n",
    "    eff,\n",
    "    N0,\n",
    "    P_AP,\n",
    "    W,\n",
    "    g_harvesting,\n",
    "    g_transmission,\n",
    "    errTol,\n",
    "    std_scaler,\n",
    "    outpt_scale_factor,\n",
    "    dnn_model,\n",
    "):\n",
    "    # g_transmission=g_transmission.T[0]\n",
    "    # g_harvesting=g_harvesting.T[0]\n",
    "    gamma = g_harvesting * g_transmission * eff * P_AP / (W * N0)\n",
    "    alpha = np.abs(scipy.special.lambertw(np.exp(-1) * (gamma - 1), k=0) + 1)\n",
    "\n",
    "    theta = D / (W * np.log2(1 + (p_max * g_transmission) / (W * N0)))  # ti at pmax\n",
    "    beta = theta * p_max / (g_harvesting * eff * P_AP)  # t0 at pmax\n",
    "\n",
    "    # C_constants=D / (W * np.log2((p_max * gamma)/ (P_AP * eff * g_harvesting) + 1));\n",
    "    # beta = (p_max * D) / (W*eff*P_AP* g_harvesting * np.log2((p_max * g_transmission)/ (N0*W) + 1));\n",
    "\n",
    "    ins = np.concatenate((g_harvesting, g_transmission, alpha, gamma))\n",
    "\n",
    "    lb = 0\n",
    "    ub = np.max(beta)\n",
    "\n",
    "    # gamma=gamma.reshape(K+N,1)\n",
    "    t0_opt, tau = bisectionSearch_dnn(\n",
    "        K + N,\n",
    "        beta,\n",
    "        theta,\n",
    "        lb,\n",
    "        ub,\n",
    "        errTol * 1e-2,\n",
    "        D,\n",
    "        gamma,\n",
    "        W,\n",
    "        ins,\n",
    "        std_scaler,\n",
    "        outpt_scale_factor,\n",
    "        dnn_model,\n",
    "    )\n",
    "    # energy_harvested=g_harvesting * eff * P_AP * t0_opt\n",
    "    # tx_power=energy_harvested/ tau\n",
    "    # tx_power[tx_power<1e-2]=1e-2\n",
    "    # data_sent=tau*W*np.log2(1+(tx_power*g_transmission)/(W*N0))\n",
    "    return t0_opt, tau"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QV92Rcjj3MaH",
    "outputId": "49195066-e827-48cc-b326-078c9470c477"
   },
   "source": [
    "avg_sub_dnn_runtime = []\n",
    "min_sub_dnn_runtime = []\n",
    "max_sub_dnn_runtime = []\n",
    "avg_sub_dnn_tottime = []\n",
    "for num_of_user in range(2, 5, 2):\n",
    "    print(num_of_user)\n",
    "    # mat = scipy.io.loadmat(input_data_folder+'/subdnn_label_N014'+'_PAP'+str(P_AP)+'_M'+str(num_of_user)+'_pmax'+str(p_max)+'.mat')\n",
    "\n",
    "    gh = match[\"gh_arr\"][0:num_of_user, 0:numChReal]\n",
    "    gt = match[\"gt_arr\"][0:num_of_user, 0:numChReal]\n",
    "\n",
    "    gh_test = match[\"gh_arr\"][0:num_of_user, test_ind].T\n",
    "    gt_test = match[\"gt_arr\"][0:num_of_user, test_ind].T\n",
    "\n",
    "    model = torch.load(output_data_folder + \"model_N\" + str(num_of_user) + \".pt\")\n",
    "    model.eval()\n",
    "    ss = pickle.load(\n",
    "        open(output_data_folder + \"scaler_N\" + str(num_of_user) + \".pkl\", \"rb\")\n",
    "    )\n",
    "\n",
    "    sub_dnn_runtime = np.zeros(len(test_ind))\n",
    "    sub_dnn_tottime = np.zeros(len(test_ind))\n",
    "    gh_test = match[\"gh_arr\"][0:num_of_user, test_ind].T\n",
    "    gt_test = match[\"gt_arr\"][0:num_of_user, test_ind].T\n",
    "    for ind in range(len(test_ind)):\n",
    "        time_start = datetime.datetime.now()\n",
    "        t0_opt, tau = MRTTMA_dnn(\n",
    "            0,\n",
    "            num_of_user,\n",
    "            D * np.ones(num_of_user),\n",
    "            p_max,\n",
    "            eff,\n",
    "            N0,\n",
    "            P_AP,\n",
    "            W,\n",
    "            gh_test[ind, :],\n",
    "            gt_test[ind, :],\n",
    "            1e-8,\n",
    "            ss,\n",
    "            scale_factor,\n",
    "            model,\n",
    "        )\n",
    "        time_stop = datetime.datetime.now()\n",
    "        data_sent_arr = calc_data_sent(\n",
    "            t0_opt, tau, gh_test[ind, :], gt_test[ind, :], D, W, eff, P_AP, N0, p_max\n",
    "        )\n",
    "        num_time_frame3 = D / data_sent_arr\n",
    "        num_time_frame3[num_time_frame3 < 1] = 1\n",
    "        sub_dnn_tottime[ind] = (\n",
    "            np.max(num_time_frame3) * t0_opt + np.multiply(num_time_frame3, tau).sum()\n",
    "        )\n",
    "        # sub_dnn_tottime[ind]=t0_opt+tau.sum()\n",
    "        sub_dnn_runtime[ind] = (time_stop - time_start).total_seconds()\n",
    "    avg_sub_dnn_runtime.append(np.mean(sub_dnn_runtime))\n",
    "    min_sub_dnn_runtime.append(np.min(sub_dnn_runtime))\n",
    "    max_sub_dnn_runtime.append(np.max(sub_dnn_runtime))\n",
    "    avg_sub_dnn_tottime.append(np.mean(sub_dnn_tottime))\n",
    "#np.savez(\"runtime_results/xai_all_trainsize\" + str(train_data_size) + \"subdnn_runtime.npz\",mean=avg_sub_dnn_runtime,min=min_sub_dnn_runtime,max=max_sub_dnn_runtime,)\n",
    "# np.save('sub_dnn_models/xai_trainsize'+str(train_data_size)+'all_tot_times_N014_PAP2_M'+str(num_of_user)+'_pmax0.01.npy',avg_sub_dnn_tottime)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EH0v7eCZ3MaJ"
   },
   "source": [
    "class SubNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SubNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(4 * num_of_user + 1, 8 * num_of_user)\n",
    "        self.layer2 = nn.Linear(8 * num_of_user, 8 * num_of_user)\n",
    "        self.layer3 = nn.Linear(8 * num_of_user, 8 * num_of_user)\n",
    "        self.layer4 = nn.Linear(8 * num_of_user, 4 * num_of_user)\n",
    "        self.layer5 = nn.Linear(4 * num_of_user, 4 * num_of_user)\n",
    "        self.output = nn.Linear(4 * num_of_user, num_of_user)\n",
    "\n",
    "        self.droplayer1 = nn.Dropout(0.5)\n",
    "        self.droplayer2 = nn.Dropout(0.5)\n",
    "        self.droplayer3 = nn.Dropout(0.5)\n",
    "        self.droplayer4 = nn.Dropout(0.5)\n",
    "        self.droplayer5 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        x = F.relu(self.layer1(input_x))\n",
    "        # x=self.droplayer1(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        # x=self.droplayer2(x)\n",
    "        x = F.relu(self.layer3(x))\n",
    "        # x=self.droplayer3(x)\n",
    "        x = F.relu(self.layer4(x))\n",
    "        # x=self.droplayer4(x)\n",
    "        x = F.relu(self.layer5(x))\n",
    "        # x=self.droplayer5(x)\n",
    "\n",
    "        x = torch.abs(self.output(x))\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NOUUIV1Y3MaJ"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
